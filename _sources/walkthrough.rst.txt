Data Reduction Walkthrough
==========================
If you have actual DKIST data then skip the Fake Data sections. Those are useful for generating testing data.

Fake Input Data
---------------
Since version *0.7.0* the pipeline assumes the input data are similar to how they will come out of the DHS. There will
be one FITS file for each SysOutput, and each file will have a single HDU with both the data and a
SPEC-0122-compliant header. It is possible to put different types of exposures (darks, gains, etc.) into separate
folders, but this is not necessary; the pipeline will segregate data based on the `DKIST004` header keyword.

For this walkthrough we will use a set of fake data generated by `visp_fake_data`, which depends on simulation data
that are too large to include in the repository. You can
`download the full set here. <https://drive.google.com/file/d/1EctElx4yAhCKQn5_NJu4akWOYeKm8EMZ/view?usp=sharing>`_
We'll assume you extract "sim_data" into the current directory. Now just run:

.. code-block:: console

  $ ./visp_fake_data -s sim_data -p 100 110 raw
  ...
  $ ls -1 raw
  VISP_20160522T001100.000000_0.fits
  VISP_20160522T001100.033333_1.fits
  VISP_20160522T001100.066667_2.fits
  VISP_20160522T001100.100000_3.fits
  VISP_20160522T001100.133334_4.fits
  VISP_20160522T001100.166667_5.fits
  VISP_20160522T001100.200000_6.fits
  VISP_20160522T001100.233334_7.fits
  VISP_20160522T001100.266667_8.fits
  VISP_20160522T001100.300001_9.fits
  ...

There are a few cool options to `visp_fake_data` that can be explored with the ``-h`` option, but two deserve special
mention: The ``-p POS1 POS2`` option only makes fake data for the range of slit positions ``[POS1, POS2)`` and can save
a ton of time; there are 1,000 slit positions included in the fake data set. The ``-L`` option is used to make
highly contrived data for a single slit position. These data have the Stokes parameter names (I, Q, U, V) as the
spectral "features". The ``-L`` option overrides the ``-p`` option.

Fake PolCal Data
----------------
We'll also need a set of ViSP data from a PolCal run. This can be generated by the
`PA&C Modules' <https://bitbucket.org/dkistdc/pac-pipeline/src/master/>`_ own `pac_fake_data`

.. code-block:: console

  $ pac_fake_data -I visp -C efficient_CS polcal
  ...
  $ ls -1 polcal
  VISP_20160522T000000.000000_0.fits
  VISP_20160522T000001.000000_1.fits
  VISP_20160522T000002.000000_2.fits
  VISP_20160522T000003.000000_3.fits
  VISP_20160522T000004.000000_4.fits
  VISP_20160522T000005.000000_5.fits
  VISP_20160522T000006.000000_6.fits
  VISP_20160522T000007.000000_7.fits
  ...

Setup
-----
Now let's create a default config file:

.. code-block:: console

  $ mkdir rdx
  $ cd rdx
  $ visp_pipeline -d raw config.ini

This should generate a good, usable config file, let's take a look:

.. code-block:: ini

  [Main]
  raw_sci_dir = raw
  data_set_id =
  output_prefix = Sci
  dark_cal = DarkCal.fits
  lamp_gain_cal = LampGainCal.fits
  solar_gain_cal = SolarGainCal.fits
  geometric_cal = GeoCal.fits
  instrument_pol_cal = InstPolCal.fits
  gen_noise_frames = False
  continuum_lims = 70:10
  save_m12_map = False
  m12_line_lims =
  threads = 3

  [DarkCalibration]
  raw_dark_dir = raw

  [LampGainCalibration]
  raw_lamp_gain_dir = raw

  [SolarGainCalibration]
  raw_solar_gain_dir = raw

  [InstrumentPolarimetricCalibration]
  raw_pol_dir = polcal
  raw_dark_dir =
  raw_lamp_gain_dir =
  raw_solar_gain_dir =
  gen_noise_frames = False
  cu_numbins_x = 1
  cu_numbins_y = 1
  demod_numbins_x = 1
  demod_numbins_y = 1
  threads = 3

.. warning::
   Generating uncertainty estimates and propagating them through the pipeline takes a lot of work. In some tests it
   more than *doubles* the run time. Only set `gen_noise_frames = True` if you really need those uncertainties.

Run
---
This is the easy part, just call the pipeline

.. code-block:: console

  $ visp_pipeline config.ini

Reducing the PolCal data takes about 15 minutes and after that each Data frame takes about 1 thread-minute to process.

The ViSP Pipeline produces a lot of status messages and it is usually a good idea to save these somewhere so you can see exactly
what was done to the data at a later time (and find any error messages). To do this I usually run the pipeline as

.. code-block:: console

  $ PYTHONUNBUFFERED=yes visp_pipeline config.ini 2>&1 | tee spool.txt

The `PYTHONUNBUFFERED=yes` is there because on most systems simply piping the python output will cause your terminal
(stdout) to update much less frequently than usual. This is due to how python decides to flush its buffer to stdout.

Output Data
-----------
If your config file looks like the one above then your reduction directory will now look like this

.. code-block:: console

  $ ls -1
  config.ini
  DarkCal.fits
  GeoCal.fits
  InstPolCal.fits
  LampGainCal.fits
  Sci.0100.fits
  Sci.0101.fits
  Sci.0102.fits
  ...
  Sci.0109.fits
  SolarGainCal.fits

All of the ``*Cal.fits`` files are the intermediate data products used by the pipeline whose names are specified in the
config file.

The processed data live in ``Sci.XXXX.fits``. These files each contain a single Primary HDU with no data and 4
ImageHDUs, one for each of the four Stokes vectors. The ``XXXX`` corresponds to the slit position (read from
the ``VISP_CSS`` header value).

Spatial Maps
------------
If a range of slit positions have been reduced (via the ``-p`` parameter, see `Input Data`_) it is possible to extract
a spatial map at a particular Stokes parameter and wavelength. The first step is to open a single reduced slit position
and determine which column corresponds to the wavelength you want (let's say the column is 123). Once that is know run:

.. code-block:: python

  >>> column = 123
  >>> stokes = 'I'
  >>> num_col_avg = 1
  >>> from ViSP_Pipeline import Data, generic
  >>> I = Data.FitsData()
  >>> I.load_polarimetric_fitsData('./','Sci*.fits',stokes)
  ...
  >>> map = generic.extract_map(I, column, num_col_avg)[0]
  ...
  >>> map.write_out('I.fits')

The file 'I.fits' will now have the spatial map of Stokes I at whatever wavelength is at column 123. The ``num_col_avg``
variable controls how many columns to average together when constructing the map.

Full Spatial Cubes
------------------
If a single wavelength isn't good enough for you and you have a lot of memory/disk space then it is also possible to
produce 3D data cubes for a given Stokes vector:

.. code-block:: python

  >>> from astropy.io import fits as pyfits
  >>> from ViSP_Pipeline import Data
  >>> Q = Data.FitsData()
  >>> Q.load_polarimetric_fitsData('./','Sci*.fits','Q')
  ...
  >>> cube = Q.get_stacked_data()
  >>> pyfits.PrimaryHDU(cube).writeto('I_cube.fits')


.. warning::
  Generating spatial maps and full cubes can be very resource intensive. If you are getting errors then check out
  :ref:`data-warning`.
