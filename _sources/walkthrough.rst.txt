Data Reduction Walkthrough
==========================

Input Data
----------
Since version *0.7.0* the pipeline assumes the input data are similar to how they will come out of the DHS. There will
be one FITS file for each SysOutput, and each file will have a single HDU with both the data and a
SPEC-0122-compliant header. It is possible to put different types of exposures (darks, gains, etc.) into separate
folders, but this is not necessary; the pipeline will segregate data based on the `DKIST004` header keyword.

For this walkthrough we will use a set of fake data generated by ``gen_fake_data.py`` (in the main repo directory),
which generates a full suite of input data:

.. code-block:: console

  $ ls -1 raw
  'VISP_2016-06-21T00:00:00.000(UTC)_0.fits'
  'VISP_2016-06-21T00:00:00.033(UTC)_1.fits'
  'VISP_2016-06-21T00:00:00.067(UTC)_2.fits'
  'VISP_2016-06-21T00:00:00.100(UTC)_3.fits'
  'VISP_2016-06-21T00:00:00.133(UTC)_4.fits'
  'VISP_2016-06-21T00:00:00.167(UTC)_5.fits'
  'VISP_2016-06-21T00:00:00.200(UTC)_6.fits'
  'VISP_2016-06-21T00:00:00.233(UTC)_7.fits'
  'VISP_2016-06-21T00:00:00.267(UTC)_8.fits'
  'VISP_2016-06-21T00:00:00.300(UTC)_9.fits'
  'VISP_2016-06-21T00:00:05.000(UTC)_0.fits'
  ...

PolCal Data
-----------
We'll also need a set of ViSP data from a PolCal run.

.. code-block:: console

  $ ls -1 polcal
  'VISP_2016-05-22T00:00:00.000(UTC)_0.fits'
  'VISP_2016-05-22T00:00:01.000(UTC)_1.fits'
  'VISP_2016-05-22T00:00:02.000(UTC)_2.fits'
  'VISP_2016-05-22T00:00:03.000(UTC)_3.fits'
  'VISP_2016-05-22T00:00:04.000(UTC)_4.fits'
  'VISP_2016-05-22T00:00:05.000(UTC)_5.fits'
  'VISP_2016-05-22T00:00:06.000(UTC)_6.fits'
  'VISP_2016-05-22T00:00:07.000(UTC)_7.fits'
  'VISP_2016-05-22T00:00:08.000(UTC)_8.fits'
  'VISP_2016-05-22T00:00:09.000(UTC)_9.fits'
  ...

Setup
-----
To keep things organized lets use a different directory to hold our pipeline outputs and create a default config file
there

.. code-block:: console

  $ mkdir rdx
  $ cd rdx
  $ visp_pipeline -b config.ini

Now we'll need to populate the config file, which should be pretty straight forward.

.. code-block:: ini

  [Main]
  raw_sci_dir = ../raw
  data_set_id =
  output_prefix = Sci
  dark_cal = DarkCal.fits
  lamp_gain_cal = LampGainCal.fits
  solar_gain_cal = SolarGainCal.fits
  geometric_cal = GeoCal.fits
  threads = 4

  [DarkCalibration]
  raw_dark_dir = ../raw

  [LampGainCalibration]
  raw_lamp_gain_dir = ../raw

  [SolarGainCalibration]
  raw_solar_gain_dir = ../raw

  [PolarimetricCalibration]
  raw_pol_dir = ../polcal
  raw_dark_dir = ../polcal
  raw_lamp_gain_dir = ../polcal
  raw_solar_gain_dir = ../polcal
  telescope_db = telescope_db.txt
  num_bins_x = 3
  num_bins_y = 1
  threads = 4

Run
---
This is the easy part, just call the pipeline

.. code-block:: console

  $ visp_pipeline config.ini

Reducing the PolCal data takes about 15 minutes and after that each Data frame takes about 1 thread-minute to process.

Output Data
-----------
If your config file looks like the one above then your reduction directory will now look like this

.. code-block:: console

  $ ls -1
  config.ini
  DarkCal.fits
  GeoCal.fits
  LampGainCal.fits
  Sci.0555.fits
  SolarGainCal.fits

All of the ``*Cal.fits`` files are the intermediate data products used by the pipeline whose names are specified in the
config file.

The processed data live in ``Sci.XXXX.fits``. These files each contain a single Primary HDU with no data and 4
ImageHDUs, one for each of the four Stokes vectors. The ``XXXX`` corresponds to the slit position (read from
the ``VISP_CSS`` header value).

Spatial Maps
------------
If (unlike the example above) a range of slit positions have been reduced it is possible to extract a spatial map at a
particular Stokes parameter and wavelength. The first step is to open a single reduced slit position and determine which
column corresponds to the wavelength you want (let's say the column is 123). Once that is know run:

.. code-block:: python

  >>> column = 123
  >>> stokes = 'I'
  >>> num_col_avg = 1
  >>> from ViSP_Pipeline import Data, generic
  >>> I = Data.FitsData()
  >>> I.load_polarimetric_fitsData('./','Sci*.fits',stokes)
  ...
  >>> map = generic.extract_map(I, column, num_col_avg)[0]
  ...
  >>> map.write_out('I.fits')

The file 'I.fits' will now have the spatial map of Stokes I at whatever wavelength is at column 123. The ``num_col_avg``
variable controls how many columns to average together when constructing the map.

Full Spatial Cubes
------------------
If a single wavelength isn't good enough for you and you have a lot of memory/disk space then it is also possible to
produce 3D data cubes for a given Stokes vector:

.. code-block:: python

  >>> from astropy.io import fits as pyfits
  >>> from ViSP_Pipeline import Data
  >>> Q = Data.FitsData()
  >>> Q.load_polarimetric_fitsData('./','Sci*.fits','Q')
  ...
  >>> cube = np.swapaxes(Q.get_stacked_data(),0,1)
  >>> pyfits..PrimaryHDU(cube).writeto('I_cube.fits')

The ``np.swapaxes`` call is necessary to format the data correctly for viewing with ds9.